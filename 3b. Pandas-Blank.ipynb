{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Pandas](https://pandas.pydata.org/)\n",
    "According to the pandas website, pandas helps fill the gap between data munging and preparation and data analysis, \"enabling you to carry out your entire data analysis workflow in Python without having to switch to a more domain specific language like R.\"\n",
    "\n",
    "Like in R, the main structure in Pandas is a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for data frames, reading and writing data\n",
    "from matplotlib import pyplot as plt\n",
    "# import psycopg2 # for connecting to a postgres database - THIS LIBRARY ISN'T LOADED\n",
    "import numpy as np # using this to create a range of floats\n",
    "from math import sqrt\n",
    "\n",
    "# the next line is so that the matplot lib plots show up in the notebook cell\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Basics\n",
    "* Create a data frame from scratch\n",
    "* Adding/removing columns\n",
    "* Descriptive information\n",
    "\n",
    "Unlike NumPy arrays and matrices, a Pandas data frame can hold different data types. However, a data frame is made up of `Pandas.Series` (columns) which must all be of the same data type. Let's create a dataframe from scratch with a few columns of different data types.\n",
    "\n",
    "*Create a data frame with three columns:*\n",
    "* 'numbers' (integers)\n",
    "* 'floats' (floats)\n",
    "* 'names' (strings)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pd.DataFrame to create the data frame. \n",
    "# You can create the data fields in a dictionary before hand, or directly in the call to pd.DataFrame\n",
    "# The dictionary should have the field name as the 'key' and the list of values as the 'value'\n",
    "df = \n",
    "\n",
    "# View The resulting dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a calculated column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a calculated column as the product of the numbers and floats: \n",
    "df['calc_col'] = \n",
    "\n",
    "#View the resulting dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some missing data so that we can look at how pandas treats it and how to find it when loading data sets later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new row as a dictionary and use np.nan for missing values. \n",
    "new_row = \n",
    "\n",
    "# Append the new row to our data frame with df.append(new_row)\n",
    "# Use \"ignore_index=True\" when appending\n",
    "df = \n",
    "\n",
    "# Reset the index to the names column after appending\n",
    "df.index = \n",
    "\n",
    "#View the resulting dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Descriptive Info\n",
    "* Column names\n",
    "* Length\n",
    "* Missing data\n",
    "\n",
    "We can use the `describe` method on a data frame to get some basic statistics on the column. The default is to only include numerical columns. Try it with and without inlclude='all' to see the different versions of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numbers:\\nmean: {:.2f}\\nstd: {:2f}\".format(df.numbers.mean(), \n",
    "                                                  df.numbers.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day to day Pandas\n",
    "Now that we've looked at some of the Pandas basics, here are some things that seem to come up regularly when using Pandas as a data engineering tool.\n",
    "\n",
    "* Reading/Writing files - xlsx, csv\n",
    "* Subsetting and merging data frames\n",
    "* Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from a Database\n",
    "We'll use some data from the Twitter work we've been doing for most of the analysis. I'll start by pulling data from our Postgres database on AWS. You cannot connect to this database without someone adding your IP address to the security group on AWS, but this will show you how to pull data from a database.\n",
    "\n",
    "NOTE: in order to connect to the Postgres database on AWS as I do in the examples below, you'll need the psycopg2 library.\n",
    "\n",
    "Let's pull 200 tweets from each topic to get a good mix. Put each into a pandas DataFrame and merge them all together. Finally, save them to Excel for the sample data for you to use. \n",
    "\n",
    "BEWARE - when pulling twitter ids (or any very large integers) into Excel, Excel tends to round them ton 15 places, losing 3 digits and making joins and merges break. Saving as csv avoids this problem, as long as you don't then view the csv in Excel.\n",
    "\n",
    "NOTE: I'm commenting all of this database stuff out, since you won't be able to connect anyway. Leaving the cells for my reference and yours, so you can see how database connections work with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Database Connection parapeters\n",
    "# hostname = 'ditwitter.c6rgtnn1vfuu.us-east-1.rds.amazonaws.com'\n",
    "# username = 'ditwitter_sa'\n",
    "# pwd = 'ThriventTwitter'\n",
    "# database = 'ditwitter'\n",
    "\n",
    "# # Connect\n",
    "# conn = psycopg2.connect( host=hostname, user=username, password=pwd, dbname=database )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First let's get a list of topics:\n",
    "# SQL = \"\"\"SELECT DISTINCT topics.* \n",
    "#         FROM topics\n",
    "#         INNER JOIN models ON md_tp_id = tp_id\"\"\"\n",
    "\n",
    "# topics_df = pd.read_sql(SQL, con=conn)\n",
    "# topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty data frame too hold the tweets we're going to collect\n",
    "# tweets_df = pd.DataFrame()\n",
    "\n",
    "# # Loop through all the active topics and grab a block of tweets, then merge with the tweets_df\n",
    "# block_size = 200\n",
    "\n",
    "# for tp_id in topics_df['tp_id']:\n",
    "#     SQL = \"\"\"SELECT t.*, tp_name \n",
    "#     FROM tweets t\n",
    "#     INNER JOIN tweet_scores ts ON ts.ts_tweet_id = t.tweet_id\n",
    "#     INNER JOIN models m ON m.md_id = ts.ts_md_id\n",
    "#     INNER JOIN topics tp ON tp.tp_id = m.md_tp_id\n",
    "#     WHERE tp_id = {}\n",
    "#     LIMIT {}\"\"\".format(tp_id, block_size)\n",
    "    \n",
    "#     tweet_block = pd.read_sql(SQL, conn)\n",
    "# #     print(\"pulled {} for topic_id: {}.\".format(len(tweet_block), tp_id))\n",
    "#     tweets_df = tweets_df.append(tweet_block)\n",
    "\n",
    "# tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Data\n",
    "Let's pull the user data for all of these records. To do that, we'll need to build a \"WHERE\" clause that has all the unique user_ids from our tweets dataframe. We'll need to convert the values to strings, then separate them by commas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the list of user_ids\n",
    "# sep = ','\n",
    "# users_string = sep.join(tweets_df['user_id'].astype(str))\n",
    "\n",
    "# SQL = \"SELECT * FROM users WHERE id in ({})\".format(users_string)\n",
    "# users_df = pd.read_sql(SQL, conn)\n",
    "# print(len(users_df))\n",
    "# users_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Excel\n",
    "Now that I've pulled the data for the examples, I'll save it to Excel for easy distribution. This is where we have to do something to get around the Excel string conversion issue. We don't care about tweet_ids, but we DO care about user_ids, since that's the field that we'll later join these two datasets on. To avoid the conversion issue, we'll add a column to the tweets_data that converts the user_id to string. We already have this column in the user data as `id_str`.\n",
    "\n",
    "*NOTE: I've commented out the save-to-excel code, since I've since added other data to the sample_data.xlsx file that I don't want overwritten. Leaving it in here for reference.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df['user_id_str'] = tweets_df['user_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "# writer = pd.ExcelWriter('sample_data.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# # Write each dataframe to a different worksheet.\n",
    "# tweets_df.to_excel(writer, sheet_name='tweet_data', index=False)\n",
    "# users_df.to_excel(writer, sheet_name='user_data', index=False)\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Read Data from Excel\n",
    "I'm using pandas to read in the data file from excel. If the file is located in the same directory as the notebook, this will work. Otherwise, add the path to the file to the filename. Pandas will automatically infer data types, column numbers and rownumbers from the data. There are quite a few different arguments that you can pass to this function to control what is loaded and how. The following cell will bring up the docstring for this function that has explanations for all of the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample_data.xlsx'\n",
    "# use pd.read_excel to read in the 'tweets_classified' sheet\n",
    "t_data = \n",
    "\n",
    "# review the data that you got\n",
    "t_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframe to tab delimited file\n",
    "Now that we have some data to work with, we can export it to a tab-delimited file. After exporting, we'll remove the data frame and reload it from the csv file.\n",
    "* setting the sep argument to '\\t' makes it tab separated. Default is comma separated\n",
    "* setting the index=False prevents it from writing out the row numbers as a column, creating an exraneous column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_filename = 'sample_data.csv'\n",
    "# use `.to_csv` to export the data to a file\n",
    "# Export to a file\n",
    "\n",
    "# destroy the object by setting it to None in case it is using a lot of memory\n",
    "t_data ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file back in to make sure everything worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file with pd.read_csv\n",
    "t_data = \n",
    "\n",
    "# View the file\n",
    "t_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an idea of what's in this dataframe - We know it has texts from different topics. Let's see how many from each are in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want the `class` variable to be binary, we have some data clean-up to do here. At some point I started using 2 for negatives, since it was easier on the keyboard than 0! Let's replace all of those 2s with 0 to make class truly binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data.loc[t_data['class']==2,'class'] = 0 \n",
    "t_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "t_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some descriptive data from this dataframe\n",
    "t_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting\n",
    "Subsetting dataframes with Pandas is very similar to subsetting in R. Since the sample data has data from 5 different topics, let's pull out two topics and make them separate data frames.\n",
    "\n",
    "Unlike R, when subsetting with Pandas you have to use `loc` or `iloc` before adding in the subset parameters. \n",
    "* `loc` is used when you have a criteria based on the values in a column or multiple columns\n",
    "* `iloc` will give you the values from a numeric position in the dataframe. For example, if you wanted the first 10 rows of the data frame, you'd do the following:\n",
    "\n",
    "*NOTE: Unlike R, Python is zero-based, so lists and indexes start at zero, rather than one.* \n",
    "#### Subsetting with `iloc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 rows\n",
    "t_data.iloc[ ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting with `loc`\n",
    "When referencing columns in pandas, you can use either dataframe.column_name or dataframe['column name']. They should work the same way. Sometimes, maybe based on the column name itself, the .column_name doesn't work. ['column_name'] seems to be more reliable. In this dataframe, I had this issue with the 'class' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "moving_df = t_data.loc[ ]\n",
    "moving_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marriage_df = t_data.loc[ ]\n",
    "marriage_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple subset criteria\n",
    "This works the same way that subsetting in R does. Let's find all of the Marriage and Moving tweets where the Class==1. A common error when subsetting is: `The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().` If you get that, make sure you are using `&` and `|` for and/or operators. If it's still an issue, check parentheses - it seems like it needs more than necessary for the subset to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .loc and the criteria that (topic=='Marriage' & class==1) or (topic=='Moving' & class==1)\n",
    "subset2 = t_data.loc[(  &  ) |\n",
    "                     (( &  )]\n",
    "subset2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data Frames\n",
    "Now that we have two separate data frames for Marriage and Moving, let's merge them together and see if the number of class==1 matches our subset above. There are a lot of options when merging data frames - similar to joins with data tables. The [documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) is pretty helpful as is StackOverflow, of course.\n",
    "\n",
    "This first example is just a combination of two dataframes... no index to match on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the marriage_df and moving_df as an outer-join. You can pick either one and .merge with the other data frame\n",
    "merged_df = marriage_df.merge(  )\n",
    "print('Marrige data frame: {}'.format(len(marriage_df)))\n",
    "print('Moving data frame: {}'.format(len(moving_df)))\n",
    "print('Merged data frame: {}'.format(len(merged_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does our count of positive tweets match between the subset and the merged data?\n",
    "len(subset2) == len(merged_df.loc[merged_df['class']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some different data to show how to merge on different keys. Our sample file has data for this too in the `tweet_data` and `user_data` sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pd.read_excel to read in the 'tweet_data' and 'user_data' sheets from the sample data spreadsheet\n",
    "tweet_df = \n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = \n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a handful of tweets, we want to merge the tweet data with the user data to append specific user columns to the tweet data. Let's only grab a few columns from each data frame to keep it easy to read. We can select a subset of columns with no other criteria with `dataframe[[list of columns]]`.\n",
    "\n",
    "The `how` parameter of the merge works like a join, defining what rows to keep when there isn't a match in both dataframes. It defaults to an inner join. In this case I want to keep all of the tweets, even if we don't have a user record, so I'm using `how=left` since the first table in the merge (the left one) is the tweet_df.\n",
    "\n",
    "NOTE: join on tweet_df.user_id_str = user_df.id_str, to avoid any truncation of the long integers that may have happend in exporting to Excel!\n",
    "\n",
    "### Merge using a unique match key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ['tweet_id', 'created_at', 'user_id_str', 'text'] from the tweet_df with\n",
    "# ['id_str', 'name','screen_name', 'followers_count'] from the user_df\n",
    "# Left outer join on tweet_df.user_id_str = user_df.id_str\n",
    "\n",
    "merged_tweets = tweet_df[ ].merge(\n",
    "    user_df[ ],\n",
    "    left_on= ,\n",
    "    right_on= ,\n",
    "    how= )\n",
    "\n",
    "\n",
    "merged_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin a continuous variable into a new variable.\n",
    "Since we have a bunch of users, let's bin their followers_count into equal width bins. This was a new one for me, but there is a handy pandas function for it, similar to R, called [pandas.cut](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets['binned_followers'] = pd.cut(merged_tweets['followers_count'], bins=5, labels = ['very_low',\n",
    "                                                                                               'low',\n",
    "                                                                                               'medium',\n",
    "                                                                                               'high',\n",
    "                                                                                              'very_high'])\n",
    "merged_tweets['binned_followers'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning with an equal number of members\n",
    "Cutting the followers_count into equal sized bins wasn't very helpful, since there are two users with so many followers that the ranges become unuseful.  \n",
    "\n",
    "More useful may be to use quartiles. For that, we'll have to calculate the quartiles ahead of time, then pass them into the `cut` function as the `squence of scalars`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 5\n",
    "# use pandas.quantile function and np.linspace to generate the cutoff values for the cut-function.\n",
    "cutoffs = list(merged_tweets['followers_count'].quantile(np.linspace(0,1,bins+1)))\n",
    "\n",
    "# create some labels for our new, binned column\n",
    "labels = ['Q'+str(x) for x in range(1,bins+1)]\n",
    "\n",
    "# cut the data based on the cutoffs\n",
    "merged_tweets['quartile_followers'] = pd.cut(merged_tweets['followers_count'], cutoffs, labels = labels)\n",
    "\n",
    "# use 'value_counts' again to check if it worked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how that worked out. We'll look at the mean, median and median and standard deviation for each quartile of follower_counts. We can use the groupby function in pandas to get these aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groupby object from merged_tweets\n",
    "# groupby 'quartile_followers'\n",
    "# aggregate the 'followers_count' column with the following funcitions: \n",
    "# count, min, max, mean, median, std\n",
    "\n",
    "merged_tweets.groupby( )[ ].agg([ ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro variable for data selection\n",
    "From Corinne: *In SAS we frequently create a list of variables by putting them in a macro variable that we use for data exploration and variable selection so that we can perform the necessary tasks for all variables easily.*\n",
    "\n",
    "Using the user_df data frame, let's pick a subset of data for our macro variable. The macro variable will be a list with the column names we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the list of all columns\n",
    "list(user_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our subset variable - let's pick all the numerical fields\n",
    "col_subset = [\n",
    "    \n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "# Now we can use this variable to select from the data:\n",
    "user_df[col_subset].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crosstab, Pivots and GroupBy\n",
    "Back to our classified tweet data, we have different topics (multiple) and we have classification (binary). We can create a two-way frequency table showing the number of each class in each topic. We can use panads.crosstab to get to this result.\n",
    "\n",
    "#### Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_way = pd.crosstab(t_data['topic'],t_data['class'])\n",
    "two_way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot Table\n",
    "You can also use the pivot_table function to get to the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_way_pivot = t_data.pivot_table(index='topic', columns = 'class', aggfunc=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these results have a multi-part index, making it a little complicated to subset the results. Since it has a multiple index, you have to pass values or criteria for both components of the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_way.loc[(['Marriage','Moving'],[0,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_way_pivot.loc[['Marriage','Moving']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pivot approach seems to be more difficult to subset, as doesn't like my second part of the index.\n",
    "\n",
    "Let's revisit our GroupBy table from the previous section and look at a pivot version of it. Here we are looking to get aggregate data for the different quartiles that we created based on follower_count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = merged_tweets.groupby('quartile_followers')['followers_count'].agg(['count','min','max','mean', 'median', 'std'])\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Pivoting\n",
    "Similar to pivot tables in Excel, Pandas creates a hierarchy based on the pivot values and then applies one or multiple aggregate functions to the values not included in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_data = user_df[['followers_count', 'friends_count', 'favourites_count', 'lang', 'statuses_count', 'time_zone', 'state']]\n",
    "pivoted = pivot_data.pivot_table(index = ['time_zone', 'state'], aggfunc=['count','mean'])\n",
    "pivoted.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting data from a dataframe with a multi-level index is more complicated than a simple data frame. This [reference](https://pandas.pydata.org/pandas-docs/stable/advanced.html) can help. Let's select two timezones from the pivot table that we just created and the mean of two of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted.loc[(['Eastern Time (US & Canada)','Central Time (US & Canada)'], 'mean')][['friends_count', 'favourites_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted2 = pivot_data.pivot_table(index = ['time_zone'], aggfunc=['count','mean'])\n",
    "pivoted2.loc[['Eastern Time (US & Canada)','Central Time (US & Canada)'], 'mean'][['friends_count', 'favourites_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to generate a summary of interval/continuous/numeric variables including\n",
    "* Basic statistics like the mean, median, percentiles, standard deviation, etc.\n",
    "* Confidence intervals around the mean\n",
    "\n",
    "A quick way to get to some of this information is with the `describe` function on a dataframe. By default, this will only describe numeric variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add `include='all'` we'll get descriptive date on the rest of the columns, with a bunch of 'NaN' for irrelevant statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tweets.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this gives you the standard deviation for numeric fields, you can use that to create confidence intervals as needed. Here's how you can pull values out of this function - let's say we want the standard deviation for the followers_count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the description to a data frame variable, then pull the value as a subset\n",
    "desc = merged_tweets.describe(include='all')\n",
    "desc.loc['std','followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or call the function and pull the value directly from the results, if there's no other need for that data.\n",
    "merged_tweets.describe().loc['std','followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double checking that it's right using the standard deviation function (std)\n",
    "merged_tweets['followers_count'].std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
