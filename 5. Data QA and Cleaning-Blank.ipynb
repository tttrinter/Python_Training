{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data QA and Cleaning\n",
    "Before we can do any meaningful analysis of any data, we need to make sure that it is the ata that we need and take care of any \"bad\" data that could be problematic. Some common things that we're looking for in data Quality Assurance (QA) are:\n",
    "* Consistent data types\n",
    "* Missing and N/A values\n",
    "* Unreasonable outliers/values\n",
    "* Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd # for data frames, reading and writing data\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# the next line is so that the matplot lib plots show up in the notebook cell\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Let's use the same sample data that we used before in the Pandas section. We'll load the user data, since that has the most fields and potential for \"dirty\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample_data.xlsx'\n",
    "user_df = \n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the data with the `describe` function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NA Columns\n",
    "Since the describe function doesn't give us the count of NA values, we can easily get that with a call to the \".isna()\" function and then sum those results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to create a model using the text from `description` as the inputs to the model, we'd want to drop all of the records with NA descriptions. We may also want to drop any descriptions with a length shorter than n-words.\n",
    "\n",
    "Let's create a subset that has descriptions with more than 2 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the description word-count (we'll use list comprehension \n",
    "# and the 'split' function for that\n",
    "\n",
    "user_df['desc_len'] = [   ]\n",
    "\n",
    "# Filter the data set to those descriptions with non-NA and length >2.\n",
    "user_sub = user_df.loc[  ]\n",
    "len(user_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "Sometimes imported data doesn't arrive as the data type that you need. Most often this happens with dates, but sometimes numbers come in as strings too, causing problems. \n",
    "\n",
    "Use the `dtypes` method on a pandas dataframe to get the list of data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sub.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the created_at date came in as a date, so no need to change it. Let's create a string version of that variable and then convert it back...\n",
    "\n",
    "We can use the .astype() function to convert an entire pandas.Series to a different type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sub['created_at_str'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we got this data with the created_at_str date as a string and we want to create a date-only column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dates = pd.to_datetime(user_sub.created_at_str, format='%Y-%m-%d')\n",
    "\n",
    "# Test to see if the convertion matches the original data:\n",
    "converted_dates.equals(user_sub['created_at'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a date only column\n",
    "user_sub.loc[:, 'created_date'] = \n",
    "user_sub.loc[:, 'created_month'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick EDA\n",
    "Pandas has connected to some of the more basic MatPlotLib plotting functionalit\n",
    "y. This makes it easier to create quick plots of dataframe data. With our new created_date, let's create a quick historgram to see how many tweets are created/day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of `created_month`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of created_month to compare with the histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging and Appending Data\n",
    "We covered merging and appending data in the Pandas notebook. Here are some stumbling blocks that I've run into when trying to merge data and ways around them:\n",
    "1. Data types - if the column data types don't match, Pandas won't merge the data. Sometimes even when you call the `.astype()` on a column, say to convert it from float to int, it won't work if there is a non-integer value in there.\n",
    "    Solution - wrestle with both data frames until you can get the datatypes to match. I've had to keep integer fields as floats, or change dates to strings in order for a merge to work.\n",
    "1. Date-data - there are a few different data types (pandas.datetime, regular datetime, others?) they have to match for a merge to work.\n",
    "1. Indices - if you are just appending data, you may need to add `ignore_index=True` for the append to work, especially if there are matching indices in the two data frames.\n",
    "1. Extra columns - these automatically get added when you append two data frames, with the resulting data frame having all of the columns existing in both.\n",
    "1. Column order - this seems to get messed up sometimes when you append or merge data frames. It's easy to fix by simply redefining the data set by passing in the list of columns in the order that you need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Missing Values\n",
    "There are many different options when looking to fill in missing values. Some common methods:\n",
    "1. Drop the data\n",
    "1. Fill with zeros\n",
    "1. Fill with the population mean\n",
    "1. Fill with a grouped mean (mean of a subset that each missing data point belongs to)\n",
    "\n",
    "All of these can be handled by subsetting the data frames and applying the logic that you want to fill the variable. \n",
    "\n",
    "We've already shown how to drop the data above when we dropped rows that had insufficient data in the `description` column for our analysis.\n",
    "\n",
    "To create some examples, let's create some holes in the user data that we've been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample from the data and set the values to na\n",
    "sample_idx = user_df.sample(n=40, random_state=24).index \n",
    "user_df.loc[sample_idx, ['followers_count', 'friends_count', 'favourites_count']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function from earlier to check our new NA counts\n",
    "user_df[['followers_count', 'friends_count', 'favourites_count']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fill values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill values with population mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fill values with grouped mean\n",
    "This one is a little tougher, since we have to calculate means for each group. We can create a grouped data frame to get those means, then merge them to update the NA values. Let's use `time_zone` as our grouping variable and populatet the `favourites_count` with the average within the time-zone.\n",
    "\n",
    "First, let's take a look at how many unique time_zone values we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .value_counts to look at unique time_zone counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we need to do some clean-up on these timezones. We can create a 'grouped_tz' column that combined the similar time zones and/or groups all the stragglers into one value. Rather than overwrite the time_zone variable, we'll use this new variable so that none of the data is lost in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of missing values! Let's mark those as missing for now:\n",
    "user_df.loc[user_df.time_zone.isna(), 'time_zone'] = 'Missing'\n",
    "\n",
    "# Create a new value for the grouped time zone so we don't lose the original data.\n",
    "user_df['grouped_tz'] = 'Other'\n",
    "user_df.loc[user_df.time_zone=='missing', 'grouped_tz'] = 'Missing'\n",
    "\n",
    "#Europe\n",
    "user_df.loc[user_df.time_zone.isin(['London', 'Dublin','Edinburgh','Amsterdam','Stockholm','Lisbon']),\n",
    "            'grouped_tz'] = 'Europe'\n",
    "# Eastern & Atlantic\n",
    "user_df.loc[user_df.time_zone.isin(['Eastern Time (US & Canada)', 'America/New_York','Indiana (East)', 'Atlantic Time (Canada)']),\n",
    "            'grouped_tz'] = 'Eastern'\n",
    "# Central\n",
    "user_df.loc[user_df.time_zone.isin(['Central Time (US & Canada)', 'America/Toronto']),\n",
    "            'grouped_tz'] = 'Central'\n",
    "# Pacific, Alaska, Hawaii\n",
    "user_df.loc[user_df.time_zone.isin(['Pacific Time (US & Canada)', 'America/Los_Angeles','Alaska','Hawaii']),\n",
    "            'grouped_tz'] = 'Pacific'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groupby object from the user_df with only grouped_tz and favorites_count columns\n",
    "# group by grouped_tz, use as_index=False to flatten the multi-index index\n",
    "# aggregate to get the means\n",
    "tz_means = user_df[ ].groupby('grouped_tz', as_index=False).agg('mean') \n",
    "\n",
    "# rename the mean column to 'favourites_mean' avoid a conflict when updating\n",
    "tz_means.columns = [  ]\n",
    "tz_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the user_df with the tz_means on the time_zone value - left outer join style so we don't lose any user_df values\n",
    "user_df_merged = user_df.merge(  )\n",
    "\n",
    "# Check the results\n",
    "user_df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the missing friends_counts with the new friends_mean:\n",
    "user_df_merged.loc[[update conditions] , 'favourites_count'] = user_df_merged.loc[  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_merged.loc[sample_idx, ['time_zone', 'favorites_mean', 'followers_count', 'friends_count', 'favourites_count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Outliers\n",
    "Sometimes we want to identify outliers in a particular data field and review them before including them in the analysis. Similar to the treatment of missing values, we may want to:\n",
    "1. Remove the outliers, or\n",
    "2. Cap the outliers at some maximum (or minimum) value\n",
    "\n",
    "Again let's look at the followers_count and review the percentiles for each of the users in our users_df dataframe. We can use the scipy stats module to calculate the percentile for each value to identify the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_followers = user_df[['id','followers_count']].sort_values('followers_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation\n",
    "fc_mean = \n",
    "fc_sd = \n",
    "print('mean = {:.2f}\\nstandard deviation = {:.2f}'.format(fc_mean, fc_sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_followers['percentile'] = stats.norm.cdf(user_df_followers['followers_count'], fc_mean, fc_sd)\n",
    "user_df_followers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the standard deviation is so large, many of the smaller values have cdf values close to 50%. Let's take a look at all of the values over the 95th percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset user_df_followers for those rows where the percentile > 0.95.\n",
    "user_df_followers.loc[  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new column for a capped follower_count so we don't lose the original data. We'll cap the value at the 95th percentile value. We can use the `stats.norm.ppf` function to get the value f the 95th percentile, effectively a reverse cdf calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capped_value = round(stats.norm.ppf(.95, fc_mean, fc_sd),0)\n",
    "user_df_followers['capped_followers'] = [ \"list comprehension to use actual value below cap and cap value above\" ]   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if it worked:\n",
    "user_df_followers.loc[user_df_followers['percentile']>0.85]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
