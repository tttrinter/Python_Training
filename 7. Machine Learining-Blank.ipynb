{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Modeling\n",
    "For this section on modeling and machine learning, we'll focus on scikit-learn as the main library. The [scikit-learn documentation](http://scikit-learn.org/stable/) is very thorough and a good source of general information about different models and applications as well as specific details about their implementation, requirements and syntax.\n",
    "\n",
    "### Model examples:\n",
    "1. Linear regression\n",
    "2. Logistic regression\n",
    "3. Vocabulary based classifier - a la Twitter topic models\n",
    "\n",
    "### Modeling steps: \n",
    "1. Data collection\n",
    "1. Data QA and cleaning\n",
    "1. Feature engineering and extraction\n",
    "1. Split data into train/test\n",
    "1. Model selection\n",
    "1. Model tuning (repeat as necessary)\n",
    "1. Model application\n",
    "1. Regularly review and re-fit models if/when they deteriorate.\n",
    "\n",
    "Something to note about scikit-learn modeling - the feature matrices are always expected to be numeric. This is directly from their [FAQ](http://scikit-learn.org/stable/faq.html):\n",
    "\n",
    "*Generally, scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.*\n",
    "\n",
    "There is a section in that same FAQ that explains how to deal with string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd # for data frames, reading and writing data\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import Ridge, SGDClassifier, Lasso\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# the next line is so that the matplot lib plots show up in the notebook cell\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Scikit-Learn comes with some built-in datasets that we can use. Let's take a look at their \"boston\" dataset, which is a dataset with house prices and features that could be predictive for house prices. The median value is typically the target. Calling `sklearn.datasets.load_boston()` returns a dictionary with keys:\n",
    "* data\n",
    "* target\n",
    "* feature_names\n",
    "* DESCR (description)\n",
    "\n",
    "To be more consistent with how we normally get data (reading from a dataset or table, rather than arrays), I've saved the `boston` data as a worksheet in our sample_data.xlsx workbook. The worksheet is called 'boston'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this data into a dateframe to be more consistent with what we normally see when pulling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
    "boston_df['target'] = boston['target']\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from Excel\n",
    "This data frame is now saved in the sample_data.xlsx file that we've been using. Load the sheet 'boston'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample_data.xlsx'\n",
    "boston_df = \n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Variable\n",
    "Let's take a quick look at the response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log of Response Variable\n",
    "Let's take a look at the log-response to see if it is potentially better for our modeling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of the log of the target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-transform of the response variable has a distribution much closer to normal. We'll create a log-target variable to see if it gives us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new variable - 'log_target' showing the log of the target\n",
    "boston_df['log_target'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Test and Train\n",
    "Before fitting a predictive model, we'll split the data into train and test data sets. This can be done using the `sample` method from pandas, or the `train_test_split` method in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = \n",
    "print('Train length: {} \\nTest Length: {}'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis\n",
    "Let's start with a PCA of this data. \n",
    "\n",
    "Here's an example in [Scikit-Learn](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate the PCA model, setting the number of components to use\n",
    "pca_unscaled = \n",
    "\n",
    "#Fit the PCA using training data\n",
    "pca_unscaled.fit(train_data.iloc[:,0:13])\n",
    "\n",
    "# Use a dataframe for easier viewing:\n",
    "pd.DataFrame(pca_unscaled.components_, columns=boston_df.columns[0:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "# Scale the training and test data\n",
    "X_train = \n",
    "X_test = \n",
    "\n",
    "# Re-run the PCA using the scaled data\n",
    "pca_scaled = \n",
    "\n",
    "#Re-Fit the model\n",
    "pca_scaled.fit(X_train)\n",
    "\n",
    "# View in a dataframe again:\n",
    "pd.DataFrame(pca_scaled.components_, columns=boston_df.columns[0:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at how much of the variance is explained by each of the principal components by looking at the `explained_variance_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pca_scaled.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(pca_unscaled.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Normally we'd next spend a good amount of time exploring the data for natural relationships, correlations, etc... However, for our purposes here, let's skip directly to fitting a linear regression model using all of the features in our dataset. Conveniently, all of the data is already numeric, so there is no data transformation required, other than normalization, which we've already done with the `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Response Variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train =  train_data['target']\n",
    "Y_test = test_data['target']\n",
    "Y_train_log = train_data['log_target']\n",
    "Y_test_log = test_data['log_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Regression Model\n",
    "We'll create a linear regression model using all of the features available for now. After fitting the model, we'll predict values for both the train and test data in order to evaluate the fit for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr1 = \n",
    "\n",
    "# Train the model using the training sets\n",
    "regr1.fit( )\n",
    "\n",
    "# Make predictions - calling the 'predict' method on the model\n",
    "Y_pred_train = \n",
    "Y_pred_test = \n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr1.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test, Y_pred_test))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Train variance score: %.2f' % r2_score(Y_train, Y_pred_train))\n",
    "print('Test variance score: %.2f' % r2_score(Y_test, Y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Regression to log-response\n",
    "Let's run it again using the log-response as our target instead and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr2 = \n",
    "\n",
    "# Train the model using the training sets\n",
    "regr2.fit( )\n",
    "\n",
    "# Make predictions \n",
    "Y_pred_log_train = \n",
    "Y_pred_log_test = \n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr2.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test_log, Y_pred_log_test))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Train variance score: %.2f' % r2_score(Y_train_log, Y_pred_log_train))\n",
    "print('Test variance score: %.2f' % r2_score(Y_test_log, Y_pred_log_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Models\n",
    "Next let's look at a classifier model using our classified life-event tweets. We'll create a classifier based on the vocabulary of the tweets. So far we've only created binary classifiers, since the tweet data has been gathered by tweet topic. However, since we have a dataset with multiple tweet categories classified, we'll create a multi-class classifier model.\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample_data.xlsx'\n",
    "t_data = pd.read_excel(filename, sheet_name='tweets_classified')\n",
    "t_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extracting the vocabulary from these texts, the data needs a little cleaning to remove some noise. Here's what we'll do:\n",
    "1. Convert everything to lower case\n",
    "1. Remove hyperlinks\n",
    "1. Stemming - cutting words down to their roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data['mod_text'] = [re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '',\n",
    "                    str(x).lower()) for x in t_data['text']]\n",
    "t_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train, t_test = \n",
    "print('Train length: {} \\nTest Length: {}'.format(len(t_train), len(t_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Vectorizer\n",
    "Now that we've 'cleaned' our data, we need to transform it into a vocabulary of words that becomes our feature matrix for scikit-learn. We'll use the `CountVectorizer` from scikit-learn along with the `SnowballStemmer` from `nltk` (natural language tool kit) to create our feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "# vectorizer = CountVectorizer(min_df=10, stop_words=stop_words, ngram_range=(1,2))\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model and learns the vocabulary;\n",
    "# second, it transforms our training data into feature vectors. \n",
    "# The input to fit_transform should be a list of strings.\n",
    "\n",
    "#creating the custom, stemmed count vectorizer\n",
    "english_stemmer = SnowballStemmer('english')\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "vectorizer_s = StemmedCountVectorizer(min_df=5, analyzer=\"word\", stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Feature Matrices\n",
    "Using the vectorizer that we just created, we can extract our feature matrices from the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature vectors for all of the different data sets.\n",
    "train_data_features = \n",
    "test_data_features = \n",
    "word_features = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7b573867-df53-4495-8c88-80b554082be0"
    }
   },
   "source": [
    "Let's see what words are used most frequently. Looking at the top 20 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "68b85f78-791e-4242-9123-e5adcb7cf826"
    }
   },
   "outputs": [],
   "source": [
    "word_counts = train_data_features.sum(axis = 0)\n",
    "word_count_df = pd.DataFrame(word_features)\n",
    "word_count_df['count'] = word_counts[0,:].tolist()[0]\n",
    "word_count_df.columns = ['word','count']\n",
    "word_count_df.sort_values(by = 'count', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response variable\n",
    "We are trying to predict the topic that each tweet belongs to. Since we have multiple topics represented, this becomes a multi-class classification problem. Let's look at the value counts for our different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c8b8d45a0c0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 't_data' is not defined"
     ]
    }
   ],
   "source": [
    "t_data.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make these text labels into targets that scikit-learn can work with, we need to transform them into binary varialbes. We can use the Scikit-learn \"LabelBinarizer\" to simplify that for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classified Model\n",
    "We'll use a suport vector machine (SVM) model to build our classifier for these tweets. The feature matrices are the inputs and the targets are the topic labels. [SVC Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(train_data_features, t_train.topic) \n",
    "train_predictions = \n",
    "test_predictions = \n",
    "  \n",
    "# model accuracy for X_test   \n",
    "train_accuracy = svm_model_linear.score(train_data_features, t_train.topic) \n",
    "test_accuracy = svm_model_linear.score(test_data_features, t_test.topic) \n",
    "print('Train accuracy: {:0.4f} \\nTest accuracy: {:0.4f}'.format(train_accuracy, test_accuracy))\n",
    "  \n",
    "# creating a confusion matrix \n",
    "cm = confusion_matrix(t_test.topic, test_predictions) \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
