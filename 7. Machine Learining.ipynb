{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Modeling\n",
    "For this section on modeling and machine learning, we'll focus on scikit-learn as the main library. The [scikit-learn documentation](http://scikit-learn.org/stable/) is very thorough and a good source of general information about different models and applications as well as specific details about their implementation, requirements and syntax.\n",
    "\n",
    "### Model examples:\n",
    "1. Linear regression\n",
    "2. Logistic regression\n",
    "3. Vocabulary based classifier - a la Twitter topic models\n",
    "\n",
    "### Modeling steps: \n",
    "1. Data collection\n",
    "1. Data QA and cleaning\n",
    "1. Feature engineering and extraction\n",
    "1. Split data into train/test\n",
    "1. Model selection\n",
    "1. Model tuning (repeat as necessary)\n",
    "1. Model application\n",
    "1. Regularly review and re-fit models if/when they deteriorate.\n",
    "\n",
    "Something to note about scikit-learn modeling - the feature matrices are always expected to be numeric. This is directly from their [FAQ](http://scikit-learn.org/stable/faq.html):\n",
    "\n",
    "*Generally, scikit-learn works on any numeric data stored as numpy arrays or scipy sparse matrices. Other types that are convertible to numeric arrays such as pandas DataFrame are also acceptable.*\n",
    "\n",
    "There is a section in that same FAQ that explains how to deal with string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd # for data frames, reading and writing data\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import Ridge, SGDClassifier, Lasso\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from scipy import stats\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# the next line is so that the matplot lib plots show up in the notebook cell\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Scikit-Learn comes with some built-in datasets that we can use. Let's take a look at their \"boston\" dataset, which is a dataset with house prices and features that could be predictive for house prices. The median value is typically the target. Calling `sklearn.datasets.load_boston()` returns a dictionary with keys:\n",
    "* data\n",
    "* target\n",
    "* feature_names\n",
    "* DESCR (description)\n",
    "\n",
    "To be more consistent with how we normally get data (reading from a dataset or table, rather than arrays), I've saved the `boston` data as a worksheet in our sample_data.xlsx workbook. The worksheet is called 'boston'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this data into a dateframe to be more consistent with what we normally see when pulling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df = pd.DataFrame(boston['data'], columns=boston['feature_names'])\n",
    "boston_df['target'] = boston['target']\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  target  \n",
       "0  396.90   4.98    24.0  \n",
       "1  396.90   9.14    21.6  \n",
       "2  392.83   4.03    34.7  \n",
       "3  394.63   2.94    33.4  \n",
       "4  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'sample_data.xlsx'\n",
    "boston_df = pd.read_excel(filename, sheet_name='boston')\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT      target  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Variable\n",
    "Let's take a quick look at the response variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26168bfc438>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# np.log(boston_df['target']).hist()\n",
    "boston_df['target'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log of Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26169a8cba8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(boston_df['target']).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-transform of the response variable has a distribution much closer to normal. We'll create a log-target variable to see if it gives us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df['log_target'] = np.log(boston_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Test and Train\n",
    "Before fitting a predictive model, we'll split the data into train and test data sets. This can be done using the `sample` method from pandas, or the `train_test_split` method in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 404 \n",
      "Test Length: 102\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(boston_df, test_size = 0.2, random_state = 24)\n",
    "print('Train length: {} \\nTest Length: {}'.format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis\n",
    "Let's start with a PCA of this data. \n",
    "\n",
    "Here's an example in [Scikit-Learn](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026228</td>\n",
       "      <td>-0.047242</td>\n",
       "      <td>0.028402</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.087663</td>\n",
       "      <td>-0.006854</td>\n",
       "      <td>0.044339</td>\n",
       "      <td>0.946034</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>-0.301681</td>\n",
       "      <td>0.023435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007903</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>-0.009505</td>\n",
       "      <td>-0.302787</td>\n",
       "      <td>-0.002940</td>\n",
       "      <td>-0.952927</td>\n",
       "      <td>0.003016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.008844</td>\n",
       "      <td>0.637314</td>\n",
       "      <td>-0.088213</td>\n",
       "      <td>-0.001064</td>\n",
       "      <td>-0.001820</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>-0.751707</td>\n",
       "      <td>0.044888</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.098999</td>\n",
       "      <td>-0.007709</td>\n",
       "      <td>-0.024032</td>\n",
       "      <td>-0.091497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024533</td>\n",
       "      <td>0.764505</td>\n",
       "      <td>-0.008801</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>0.641363</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>-0.018597</td>\n",
       "      <td>-0.019871</td>\n",
       "      <td>-0.031893</td>\n",
       "      <td>0.005442</td>\n",
       "      <td>0.040971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.742108</td>\n",
       "      <td>0.012940</td>\n",
       "      <td>-0.008041</td>\n",
       "      <td>-0.004809</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>-0.045043</td>\n",
       "      <td>-0.077646</td>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.175906</td>\n",
       "      <td>-0.031470</td>\n",
       "      <td>0.051665</td>\n",
       "      <td>0.016841</td>\n",
       "      <td>0.637201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0  0.026228 -0.047242  0.028402 -0.000048  0.000464 -0.001269  0.087663   \n",
       "1  0.007903  0.003434 -0.005164 -0.000113  0.000018  0.000664 -0.006384   \n",
       "2 -0.008844  0.637314 -0.088213 -0.001064 -0.001820  0.004587 -0.751707   \n",
       "3  0.024533  0.764505 -0.008801  0.000904  0.000702  0.006172  0.641363   \n",
       "4  0.742108  0.012940 -0.008041 -0.004809 -0.000087 -0.045043 -0.077646   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0 -0.006854  0.044339  0.946034  0.005681 -0.301681  0.023435  \n",
       "1  0.000531 -0.009505 -0.302787 -0.002940 -0.952927  0.003016  \n",
       "2  0.044888  0.007625  0.098999 -0.007709 -0.024032 -0.091497  \n",
       "3  0.000993 -0.018597 -0.019871 -0.031893  0.005442  0.040971  \n",
       "4  0.007140  0.175906 -0.031470  0.051665  0.016841  0.637201  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_unscaled = PCA(n_components=5)\n",
    "pca_unscaled.fit(train_data.iloc[:,0:13])\n",
    "\n",
    "# Throwing the components into a dataframe for easier viewing:\n",
    "pd.DataFrame(pca_unscaled.components_, columns=boston_df.columns[0:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.268612</td>\n",
       "      <td>-0.254254</td>\n",
       "      <td>0.343348</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>0.339347</td>\n",
       "      <td>-0.194932</td>\n",
       "      <td>0.311258</td>\n",
       "      <td>-0.319520</td>\n",
       "      <td>0.317596</td>\n",
       "      <td>0.340041</td>\n",
       "      <td>0.198058</td>\n",
       "      <td>-0.207057</td>\n",
       "      <td>0.307489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.287317</td>\n",
       "      <td>-0.279616</td>\n",
       "      <td>0.126474</td>\n",
       "      <td>0.470373</td>\n",
       "      <td>0.239156</td>\n",
       "      <td>0.204435</td>\n",
       "      <td>0.309355</td>\n",
       "      <td>-0.350665</td>\n",
       "      <td>-0.249273</td>\n",
       "      <td>-0.207574</td>\n",
       "      <td>-0.367623</td>\n",
       "      <td>0.188243</td>\n",
       "      <td>-0.092501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.272327</td>\n",
       "      <td>0.326472</td>\n",
       "      <td>-0.046165</td>\n",
       "      <td>0.243163</td>\n",
       "      <td>0.084325</td>\n",
       "      <td>0.568979</td>\n",
       "      <td>-0.039080</td>\n",
       "      <td>-0.012770</td>\n",
       "      <td>0.305675</td>\n",
       "      <td>0.227389</td>\n",
       "      <td>-0.272019</td>\n",
       "      <td>-0.367498</td>\n",
       "      <td>-0.266953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009136</td>\n",
       "      <td>-0.022565</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.724935</td>\n",
       "      <td>-0.169389</td>\n",
       "      <td>-0.074914</td>\n",
       "      <td>-0.165645</td>\n",
       "      <td>0.129899</td>\n",
       "      <td>0.204632</td>\n",
       "      <td>0.143371</td>\n",
       "      <td>0.484850</td>\n",
       "      <td>0.295650</td>\n",
       "      <td>-0.100452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.140537</td>\n",
       "      <td>0.377696</td>\n",
       "      <td>0.013959</td>\n",
       "      <td>0.390362</td>\n",
       "      <td>0.080006</td>\n",
       "      <td>-0.499438</td>\n",
       "      <td>-0.056111</td>\n",
       "      <td>0.201711</td>\n",
       "      <td>-0.135724</td>\n",
       "      <td>-0.067917</td>\n",
       "      <td>-0.420580</td>\n",
       "      <td>-0.185586</td>\n",
       "      <td>0.388561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0  0.268612 -0.254254  0.343348  0.006382  0.339347 -0.194932  0.311258   \n",
       "1 -0.287317 -0.279616  0.126474  0.470373  0.239156  0.204435  0.309355   \n",
       "2  0.272327  0.326472 -0.046165  0.243163  0.084325  0.568979 -0.039080   \n",
       "3  0.009136 -0.022565  0.015896  0.724935 -0.169389 -0.074914 -0.165645   \n",
       "4  0.140537  0.377696  0.013959  0.390362  0.080006 -0.499438 -0.056111   \n",
       "\n",
       "        DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
       "0 -0.319520  0.317596  0.340041  0.198058 -0.207057  0.307489  \n",
       "1 -0.350665 -0.249273 -0.207574 -0.367623  0.188243 -0.092501  \n",
       "2 -0.012770  0.305675  0.227389 -0.272019 -0.367498 -0.266953  \n",
       "3  0.129899  0.204632  0.143371  0.484850  0.295650 -0.100452  \n",
       "4  0.201711 -0.135724 -0.067917 -0.420580 -0.185586  0.388561  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_train = X_scaler.fit_transform(train_data.iloc[:,0:13])\n",
    "X_test = X_scaler.transform(test_data.iloc[:,0:13])\n",
    "\n",
    "pca_scaled = PCA(n_components=5)\n",
    "pca_scaled.fit(X_train)\n",
    "\n",
    "# Throwing the components into a dataframe for easier viewing:\n",
    "pd.DataFrame(pca_scaled.components_, columns=boston_df.columns[0:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4770171492341357,\n",
       " 0.1133582007768713,\n",
       " 0.09601732783094474,\n",
       " 0.06612588020095622,\n",
       " 0.06222102502903388]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pca_scaled.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80556360706161,\n",
       " 0.16271528284577977,\n",
       " 0.02182762745157756,\n",
       " 0.007524745311693138,\n",
       " 0.0008927977491543757]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pca_unscaled.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Normally we'd next spend a good amount of time exploring the data for natural relationships, correlations, etc... However, for our purposes here, let's skip directly to fitting a linear regression model using all of the features in our dataset. Conveniently, all of the data is already numeric, so there is no data transformation required, other than normalization, which we've already done with the `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Response Variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =  train_data['target']\n",
    "y_test = test_data['target']\n",
    "y_train_log = train_data['log_target']\n",
    "y_test_log = test_data['log_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Regression Model\n",
    "* Using `statsmodels.api`\n",
    "* Using `scikit-learn`\n",
    "\n",
    "#### statsmodels.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   R-squared:                       0.754\n",
      "Model:                            OLS   Adj. R-squared:                  0.746\n",
      "Method:                 Least Squares   F-statistic:                     92.00\n",
      "Date:                Fri, 05 Oct 2018   Prob (F-statistic):          5.06e-110\n",
      "Time:                        14:10:45   Log-Likelihood:                -1195.3\n",
      "No. Observations:                 404   AIC:                             2419.\n",
      "Df Residuals:                     390   BIC:                             2475.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         22.7248      0.236     96.228      0.000      22.260      23.189\n",
      "x1            -0.3877      0.343     -1.129      0.259      -1.063       0.287\n",
      "x2             1.0197      0.353      2.885      0.004       0.325       1.715\n",
      "x3            -0.0215      0.473     -0.045      0.964      -0.951       0.908\n",
      "x4             0.8246      0.246      3.358      0.001       0.342       1.307\n",
      "x5            -1.7652      0.498     -3.545      0.000      -2.744      -0.786\n",
      "x6             3.1172      0.337      9.260      0.000       2.455       3.779\n",
      "x7            -0.1712      0.420     -0.407      0.684      -0.997       0.655\n",
      "x8            -3.0535      0.475     -6.425      0.000      -3.988      -2.119\n",
      "x9             2.2145      0.649      3.410      0.001       0.938       3.491\n",
      "x10           -2.0248      0.705     -2.872      0.004      -3.411      -0.639\n",
      "x11           -1.8410      0.319     -5.770      0.000      -2.468      -1.214\n",
      "x12            1.0124      0.279      3.624      0.000       0.463       1.562\n",
      "x13           -3.6361      0.405     -8.975      0.000      -4.433      -2.840\n",
      "==============================================================================\n",
      "Omnibus:                      129.011   Durbin-Watson:                   2.129\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              501.773\n",
      "Skew:                           1.375   Prob(JB):                    1.10e-109\n",
      "Kurtosis:                       7.716   Cond. No.                         9.85\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X2 = sm.add_constant(X_train)\n",
    "est = sm.OLS(y_train, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:             log_target   R-squared:                       0.799\n",
      "Model:                            OLS   Adj. R-squared:                  0.793\n",
      "Method:                 Least Squares   F-statistic:                     119.4\n",
      "Date:                Fri, 05 Oct 2018   Prob (F-statistic):          4.76e-127\n",
      "Time:                        14:28:56   Log-Likelihood:                 107.65\n",
      "No. Observations:                 404   AIC:                            -187.3\n",
      "Df Residuals:                     390   BIC:                            -131.3\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.0407      0.009    323.947      0.000       3.022       3.059\n",
      "x1            -0.0683      0.014     -5.004      0.000      -0.095      -0.041\n",
      "x2             0.0256      0.014      1.821      0.069      -0.002       0.053\n",
      "x3             0.0102      0.019      0.540      0.589      -0.027       0.047\n",
      "x4             0.0297      0.010      3.048      0.002       0.011       0.049\n",
      "x5            -0.0787      0.020     -3.977      0.000      -0.118      -0.040\n",
      "x6             0.0718      0.013      5.363      0.000       0.045       0.098\n",
      "x7             0.0040      0.017      0.239      0.811      -0.029       0.037\n",
      "x8            -0.0998      0.019     -5.283      0.000      -0.137      -0.063\n",
      "x9             0.1132      0.026      4.385      0.000       0.062       0.164\n",
      "x10           -0.1098      0.028     -3.916      0.000      -0.165      -0.055\n",
      "x11           -0.0738      0.013     -5.820      0.000      -0.099      -0.049\n",
      "x12            0.0379      0.011      3.415      0.001       0.016       0.060\n",
      "x13           -0.2123      0.016    -13.187      0.000      -0.244      -0.181\n",
      "==============================================================================\n",
      "Omnibus:                       49.264   Durbin-Watson:                   2.180\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              160.243\n",
      "Skew:                           0.517   Prob(JB):                     1.60e-35\n",
      "Kurtosis:                       5.907   Cond. No.                         9.85\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X2 = sm.add_constant(X_train)\n",
    "est = sm.OLS(y_train_log, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-0.38768528  1.01967098 -0.0215033   0.82462967 -1.76524459  3.11720791\n",
      " -0.17115482 -3.05354928  2.21452555 -2.02483947 -1.84103466  1.01244466\n",
      " -3.63606954]\n",
      "Mean squared error: 23.69\n",
      "Train variance score: 0.75\n",
      "Test variance score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr1 = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr1.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = regr1.predict(X_train)\n",
    "y_pred_test = regr1.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr1.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test, y_pred_test))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Train variance score: %.2f' % r2_score(y_train, y_pred_train))\n",
    "print('Test variance score: %.2f' % r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is apparently not a similar report to the one from the statsmodels.api. However we can reproduce the table with p-values as done in this post on [Stackoverflow](https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_report(X, y, model):\n",
    "    params = np.append(model.intercept_,model.coef_)\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X))\n",
    "    MSE = (sum((y-predictions)**2))/(len(newX)-len(newX.columns))\n",
    "    R2 = r2_score(y, predictions)\n",
    "\n",
    "    # Note if you don't want to use a DataFrame replace the two lines above with\n",
    "    # newX = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    # MSE = (sum((y-predictions)**2))/(len(newX)-len(newX[0]))\n",
    "\n",
    "    var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params/ sd_b\n",
    "\n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "\n",
    "    summary_df = pd.DataFrame()\n",
    "    summary_df[\"Coefficients\"],summary_df[\"Standard Errors\"],summary_df[\"t values\"],summary_df[\"Probabilites\"] = [params,sd_b,ts_b,p_values]\n",
    "    report = {'r2': R2, 'mse': MSE, 'summary': summary_df }\n",
    "    return(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 0.7540976665084576,\n",
       " 'mse': 22.53062632158762,\n",
       " 'summary':     Coefficients  Standard Errors  t values  Probabilites\n",
       " 0        22.7248            0.236    96.228         0.000\n",
       " 1        -0.3877            0.343    -1.129         0.259\n",
       " 2         1.0197            0.353     2.885         0.004\n",
       " 3        -0.0215            0.473    -0.045         0.964\n",
       " 4         0.8246            0.246     3.358         0.001\n",
       " 5        -1.7652            0.498    -3.545         0.000\n",
       " 6         3.1172            0.337     9.260         0.000\n",
       " 7        -0.1712            0.420    -0.407         0.684\n",
       " 8        -3.0535            0.475    -6.425         0.000\n",
       " 9         2.2145            0.649     3.410         0.001\n",
       " 10       -2.0248            0.705    -2.872         0.004\n",
       " 11       -1.8410            0.319    -5.770         0.000\n",
       " 12        1.0124            0.279     3.624         0.000\n",
       " 13       -3.6361            0.405    -8.975         0.000}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_report(X_train, y_train, regr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Regression to log-response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-0.0682741   0.02557869  0.01015132  0.02974944 -0.07871488  0.07175371\n",
      "  0.00399925 -0.09980754  0.11318973 -0.10975732 -0.07381239  0.0379177\n",
      " -0.21234728]\n",
      "Mean squared error: 0.04\n",
      "Train variance score: 0.80\n",
      "Test variance score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "regr2 = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr2.fit(X_train, Y_train_log)\n",
    "\n",
    "# Make predictions \n",
    "Y_pred_log_train = regr2.predict(X_train)\n",
    "Y_pred_log_test = regr2.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr2.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(Y_test_log, Y_pred_log_test))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Train variance score: %.2f' % r2_score(Y_train_log, Y_pred_log_train))\n",
    "print('Test variance score: %.2f' % r2_score(Y_test_log, Y_pred_log_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'r2': 0.7992106882401949,\n",
       " 'mse': 0.035594981557699336,\n",
       " 'summary':     Coefficients  Standard Errors  t values  Probabilites\n",
       " 0         3.0407            0.009   323.947         0.000\n",
       " 1        -0.0683            0.014    -5.004         0.000\n",
       " 2         0.0256            0.014     1.821         0.069\n",
       " 3         0.0102            0.019     0.540         0.589\n",
       " 4         0.0297            0.010     3.048         0.002\n",
       " 5        -0.0787            0.020    -3.977         0.000\n",
       " 6         0.0718            0.013     5.363         0.000\n",
       " 7         0.0040            0.017     0.239         0.811\n",
       " 8        -0.0998            0.019    -5.283         0.000\n",
       " 9         0.1132            0.026     4.385         0.000\n",
       " 10       -0.1098            0.028    -3.916         0.000\n",
       " 11       -0.0738            0.013    -5.820         0.000\n",
       " 12        0.0379            0.011     3.415         0.001\n",
       " 13       -0.2123            0.016   -13.187         0.000}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_report(X_train, Y_train_log, regr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Models\n",
    "Next let's look at a classifier model using our classified life-event tweets. We'll create a classifier based on the vocabulary of the tweets. So far we've only created binary classifiers, since the tweet data has been gathered by tweet topic. However, since we have a dataset with multiple tweet categories classified, we'll create a multi-class classifier model.\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You remind me of my BimmerSee your ignition  b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @JaDineNATION: Were so excited for our dese...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i always get super self conscious about keepin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Juhhhhhhnelle Weird ass bitch lucky Im pregna...</td>\n",
       "      <td>1</td>\n",
       "      <td>Birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @thdmichaelbell: New Signage above our Fron...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class  topic\n",
       "0  You remind me of my BimmerSee your ignition  b...      0  Birth\n",
       "1  RT @JaDineNATION: Were so excited for our dese...      0  Birth\n",
       "2  i always get super self conscious about keepin...      0  Birth\n",
       "3  @Juhhhhhhnelle Weird ass bitch lucky Im pregna...      1  Birth\n",
       "4  RT @thdmichaelbell: New Signage above our Fron...      0  Birth"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'sample_data.xlsx'\n",
    "t_data = pd.read_excel(filename, sheet_name='tweets_classified')\n",
    "t_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extracting the vocabulary from these texts, the data needs a little cleaning to remove some noise. Here's what we'll do:\n",
    "1. Convert everything to lower case\n",
    "1. Remove hyperlinks\n",
    "1. Stemming - cutting words down to their roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>topic</th>\n",
       "      <th>mod_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You remind me of my BimmerSee your ignition  b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "      <td>you remind me of my bimmersee your ignition  b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @JaDineNATION: Were so excited for our dese...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "      <td>rt @jadinenation: were so excited for our dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i always get super self conscious about keepin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "      <td>i always get super self conscious about keepin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Juhhhhhhnelle Weird ass bitch lucky Im pregna...</td>\n",
       "      <td>1</td>\n",
       "      <td>Birth</td>\n",
       "      <td>@juhhhhhhnelle weird ass bitch lucky im pregna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @thdmichaelbell: New Signage above our Fron...</td>\n",
       "      <td>0</td>\n",
       "      <td>Birth</td>\n",
       "      <td>rt @thdmichaelbell: new signage above our fron...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class  topic  \\\n",
       "0  You remind me of my BimmerSee your ignition  b...      0  Birth   \n",
       "1  RT @JaDineNATION: Were so excited for our dese...      0  Birth   \n",
       "2  i always get super self conscious about keepin...      0  Birth   \n",
       "3  @Juhhhhhhnelle Weird ass bitch lucky Im pregna...      1  Birth   \n",
       "4  RT @thdmichaelbell: New Signage above our Fron...      0  Birth   \n",
       "\n",
       "                                            mod_text  \n",
       "0  you remind me of my bimmersee your ignition  b...  \n",
       "1  rt @jadinenation: were so excited for our dese...  \n",
       "2  i always get super self conscious about keepin...  \n",
       "3  @juhhhhhhnelle weird ass bitch lucky im pregna...  \n",
       "4  rt @thdmichaelbell: new signage above our fron...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data['mod_text'] = [re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '',\n",
    "                    str(x).lower()) for x in t_data['text']]\n",
    "t_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 905 \n",
      "Test Length: 227\n"
     ]
    }
   ],
   "source": [
    "t_train, t_test = train_test_split(t_data, test_size = 0.2, random_state = 24)\n",
    "print('Train length: {} \\nTest Length: {}'.format(len(t_train), len(t_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count Vectorizer\n",
    "Now that we've 'cleaned' our data, we need to transform it into a vocabulary of words that becomes our feature matrix for scikit-learn. We'll use the `CountVectorizer` from scikit-learn along with the `SnowballStemmer` from `nltk` (natural language tool kit) to create our feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "# vectorizer = CountVectorizer(min_df=10, stop_words=stop_words, ngram_range=(1,2))\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model and learns the vocabulary;\n",
    "# second, it transforms our training data into feature vectors. \n",
    "# The input to fit_transform should be a list of strings.\n",
    "\n",
    "#creating the custom, stemmed count vectorizer\n",
    "english_stemmer = SnowballStemmer('english')\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "vectorizer_s = StemmedCountVectorizer(min_df=5, analyzer=\"word\", stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature vectors for all of the different data sets.\n",
    "train_data_features = vectorizer_s.fit_transform(t_train['mod_text'])\n",
    "test_data_features = vectorizer_s.transform(t_test['mod_text'])\n",
    "word_features = vectorizer_s.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7b573867-df53-4495-8c88-80b554082be0"
    }
   },
   "source": [
    "Let's see what words are used most frequently. Looking at the top 20 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "68b85f78-791e-4242-9123-e5adcb7cf826"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>wed</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>graduat</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>divorc</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>new</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>babi</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>home</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>rt</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>move</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>love</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>girl</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>day</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amp</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>today</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>just</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>like</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>friend</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>get</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>year</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>make</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  count\n",
       "227       wed    188\n",
       "92    graduat    187\n",
       "57     divorc    178\n",
       "147       new    166\n",
       "12       babi    117\n",
       "104      home    117\n",
       "176        rt    112\n",
       "145      move     83\n",
       "130      love     70\n",
       "84       girl     62\n",
       "54        day     62\n",
       "6         amp     56\n",
       "211     today     49\n",
       "115      just     43\n",
       "123      like     42\n",
       "162  pregnant     37\n",
       "79     friend     35\n",
       "82        get     34\n",
       "240      year     30\n",
       "132      make     30"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = train_data_features.sum(axis = 0)\n",
    "word_count_df = pd.DataFrame(word_features)\n",
    "word_count_df['count'] = word_counts[0,:].tolist()[0]\n",
    "word_count_df.columns = ['word','count']\n",
    "word_count_df.sort_values(by = 'count', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response variable\n",
    "We are trying to predict the topic that each tweet belongs to. Since we have multiple topics represented, this becomes a multi-class classification problem. Let's look at the value counts for our different topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marriage      241\n",
       "Moving        240\n",
       "Graduation    226\n",
       "Divorce       226\n",
       "Birth         199\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_data.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make these text labels into targets that scikit-learn can work with, we need to transform them into binary varialbes. We can use the Scikit-learn \"LabelBinarizer\" to simplify that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9945 \n",
      "Test accuracy: 0.9648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[37,  0,  0,  0,  1],\n",
       "       [ 0, 41,  0,  1,  0],\n",
       "       [ 2,  0, 48,  0,  1],\n",
       "       [ 0,  1,  0, 45,  0],\n",
       "       [ 1,  1,  0,  0, 48]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(train_data_features, t_train.topic) \n",
    "train_predictions = svm_model_linear.predict(train_data_features) \n",
    "test_predictions = svm_model_linear.predict(test_data_features) \n",
    "  \n",
    "# model accuracy for X_test   \n",
    "train_accuracy = svm_model_linear.score(train_data_features, t_train.topic) \n",
    "test_accuracy = svm_model_linear.score(test_data_features, t_test.topic) \n",
    "print('Train accuracy: {:0.4f} \\nTest accuracy: {:0.4f}'.format(train_accuracy, test_accuracy))\n",
    "  \n",
    "# creating a confusion matrix \n",
    "cm = confusion_matrix(t_test.topic, test_predictions) \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graduation    51\n",
       "Moving        50\n",
       "Marriage      46\n",
       "Divorce       42\n",
       "Birth         38\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test.topic.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 227 entries, 503 to 42\n",
      "Data columns (total 4 columns):\n",
      "text        227 non-null object\n",
      "class       227 non-null int64\n",
      "topic       227 non-null object\n",
      "mod_text    227 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 8.9+ KB\n"
     ]
    }
   ],
   "source": [
    "t_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 3], [2, 4]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.37228132,  5.37228132]), array([[-0.90937671, -0.56576746],\n",
       "        [ 0.41597356, -0.82456484]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen values: [-0.37228132  5.37228132] \n",
      "Eigen vectors: [[-0.90937671 -0.56576746]\n",
      " [ 0.41597356 -0.82456484]] \n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import eig\n",
    "a = np.array([[1, 3], [2, 4]])\n",
    "eigen = eig(a)\n",
    "print('Eigen values: {} '.format(eigen[0]))\n",
    "print('Eigen vectors: {} '.format(eigen[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.37228132,  5.37228132]), array([[-0.90937671, -0.56576746],\n",
       "        [ 0.41597356, -0.82456484]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ecfa78366ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meigen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}, {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "for i in eigen:\n",
    "    print('{}, {}'.format(type(i), i.shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
